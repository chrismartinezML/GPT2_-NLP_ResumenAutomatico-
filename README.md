Generaci√≥n de Res√∫menes Autom√°ticos con GPT-2: Un Proyecto de NLP Paso a Paso

Paso 1: Extracci√≥n de texto desde un archivo PDF üìÑ
El primer paso fue extraer el texto de un archivo PDF utilizando la librer√≠a pdfplumber

Paso 2: Limpieza del texto üßπ
Una vez que obtuve el texto del PDF, lo limpi√© utilizando expresiones regulares (re) para eliminar URLs, referencias a notas y caracteres especiales. Esto asegura que el modelo trabaje solo con contenido relevante y libre de ruido.

Paso 3: Tokenizaci√≥n del texto üîë
Para que el modelo pueda entender y trabajar con el texto, utilic√© la librer√≠a nltk para dividir el texto en oraciones. Esto facilita la manipulaci√≥n del texto y la posterior generaci√≥n de res√∫menes.

Paso 4: Cargar y usar el modelo GPT-2 üß†
Despu√©s de limpiar y preparar el texto, utilic√© GPT-2, un modelo de lenguaje preentrenado de OpenAI, para generar res√∫menes. Utilic√© la librer√≠a transformers de Hugging Face para cargar y trabajar con el modelo. GPT-2 es muy potente para tareas de generaci√≥n de texto.

Paso 5: Generaci√≥n de Res√∫menes Autom√°ticos ‚ö°
Finalmente, utilic√© GPT-2 para generar un resumen del texto de entrada. Al usar el m√©todo generate(), defin√≠ par√°metros como max_length para controlar la longitud del resumen generado.


Librer√≠as Utilizadas üìö
pdfplumber: Permite la extracci√≥n de texto de PDFs, incluso con estructuras complejas.

re (expresiones regulares): Utilizado para limpiar el texto, eliminar URLs, y realizar ajustes.

nltk: Librer√≠a popular para trabajar con el procesamiento de lenguaje natural, especialmente √∫til para tokenizaci√≥n.

transformers: Una de las librer√≠as m√°s potentes para trabajar con modelos preentrenados como GPT-2, BART y T5, proporcionada por Hugging Face.

torch: Framework de aprendizaje profundo que permite ejecutar el modelo en GPU para acelerar el procesamiento.
